{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:gbk\n",
    "import jieba\n",
    "import gensim\n",
    "import re\n",
    "import os\n",
    "\n",
    "jieba.setLogLevel(jieba.logging.INFO)\n",
    "\n",
    "# 获取指定路径的文件内容\n",
    "def get_file_contents(path):\n",
    "    str = ''\n",
    "    f = open(path, 'r', encoding='UTF-8')\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        str = str + line\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    return str\n",
    "\n",
    "#将读取到的文件内容先进行jieba分词，然后再把标点符号、转义符号等特殊符号过滤掉\n",
    "def filter(str):\n",
    "    str = jieba.lcut(str)\n",
    "    result = []\n",
    "    for tags in str:\n",
    "        if (re.match(u\"[a-zA-Z0-9\\u4e00-\\u9fa5]\", tags)):\n",
    "            result.append(tags)\n",
    "        else:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "# 忽略掉文本的语法和语序等要素，将其仅仅看作是若干个词汇的集合\n",
    "def convert_corpus(text1,text2):\n",
    "    texts=[text1,text2]\n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return corpus\n",
    "\n",
    "#传入过滤之后的数据，通过调用gensim.similarities.Similarity计算余弦相似度\n",
    "def calc_similarity(text1,text2):\n",
    "    texts=[text1,text2]\n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    similarity = gensim.similarities.Similarity('-Similarity-index', corpus, num_features=len(dictionary))\n",
    "    test_corpus_1 = dictionary.doc2bow(text1)\n",
    "    cosine_sim = similarity[test_corpus_1][1]\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path1 = input(\"输入论文原文的文件的绝对路径：\")\n",
    "    path2 = input(\"输入抄袭版论文的文件的绝对路径：\")\n",
    "    if not os.path.exists(path1):\n",
    "        print(\"论文原文文件不存在！\")\n",
    "        exit()\n",
    "    if not os.path.exists(path2):\n",
    "        print(\"抄袭版论文文件不存在！\")\n",
    "        exit()\n",
    "    #输出结果绝对路径\n",
    "    save_path =\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\copy\\\\result.txt\"\n",
    "    str1 = get_file_contents(path1)\n",
    "    str2 = get_file_contents(path2)\n",
    "    text1 = filter(str1)\n",
    "    text2 = filter(str2)\n",
    "    similarity = calc_similarity(text1, text2)\n",
    "    print(\"文章相似度： %.2f\"%similarity)\n",
    "    #将相似度结果写入指定文件\n",
    "    f = open(save_path, 'w', encoding=\"utf-8\")\n",
    "    f.write(\"python\"+\" \"+\"main.py\"+\" \"+path1+\" \"+path2+\" \"+\"文章相似度： %.2f\"%similarity)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import math\n",
    "import re\n",
    "#读入两个txt文件存入s1,s2字符串中\n",
    "# s1 = open('../文本分析/text/2020.txt','r').read()\n",
    "# s2 = open('../文本分析/text/2021.txt','r').read()\n",
    "s1 = open('orig_0.8_del.txt','rb+').read()\n",
    "s2 = open('orig_0.8_add.txt','rb+').read()\n",
    "\n",
    "#利用jieba分词与停用词表，将词分好并保存到向量中\n",
    "stopwords=[]\n",
    "fstop=open('orig.txt','r',encoding='utf-8-sig')\n",
    "for eachWord in fstop:\n",
    "    eachWord = re.sub(\"\\n\", \"\", eachWord)\n",
    "    stopwords.append(eachWord)\n",
    "fstop.close()\n",
    "s1_cut = [i for i in jieba.cut(s1, cut_all=True) if (i not in stopwords) and i!='']\n",
    "s2_cut = [i for i in jieba.cut(s2, cut_all=True) if (i not in stopwords) and i!='']\n",
    "word_set = set(s1_cut).union(set(s2_cut))\n",
    "\n",
    "#用字典保存两篇文章中出现的所有词并编上号\n",
    "word_dict = dict()\n",
    "i = 0\n",
    "for word in word_set:\n",
    "    word_dict[word] = i\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#根据词袋模型统计词在每篇文档中出现的次数，形成向量\n",
    "s1_cut_code = [0]*len(word_dict)\n",
    "\n",
    "for word in s1_cut:\n",
    "    s1_cut_code[word_dict[word]]+=1\n",
    "\n",
    "s2_cut_code = [0]*len(word_dict)\n",
    "for word in s2_cut:\n",
    "    s2_cut_code[word_dict[word]]+=1\n",
    "\n",
    "# 计算余弦相似度\n",
    "sum = 0\n",
    "sq1 = 0\n",
    "sq2 = 0\n",
    "for i in range(len(s1_cut_code)):\n",
    "    sum += s1_cut_code[i] * s2_cut_code[i]\n",
    "    sq1 += pow(s1_cut_code[i], 2)\n",
    "    sq2 += pow(s2_cut_code[i], 2)\n",
    "\n",
    "try:\n",
    "    result = round(float(sum) / (math.sqrt(sq1) * math.sqrt(sq2)), 3)\n",
    "except ZeroDivisionError:\n",
    "    result = 0.0\n",
    "print(\"\\n余弦相似度为：%f\"%result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5333707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def short_analyse(o_file, c_file):\n",
    "    \"\"\"\n",
    "    :读取数据然后对数据进行一个jieba分词，同时根据正则匹配来过滤掉标点\n",
    "    :param o_file: 原始论文的地址\n",
    "    :param c_file: 抄袭论文的地址\n",
    "    :return: 返回两个分词结果的列表\n",
    "    \"\"\"\n",
    "    jieba.setLogLevel(jieba.logging.INFO)  # 使用中文词库来进行分词，防止报错\n",
    "    o_list = []\n",
    "    c_list = []\n",
    "    try:\n",
    "        with open(o_file, 'r', encoding='utf-8') as f:\n",
    "            o_lines = f.readlines()\n",
    "        for line in o_lines:\n",
    "            pattern = re.compile(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")  # 正则匹配保留中文字符\n",
    "            target = pattern.sub(\"\", line)\n",
    "            for data in jieba.lcut(target):\n",
    "                o_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{o_file}这个路径下没有文件\")\n",
    "\n",
    "    try:\n",
    "        with open(c_file, 'r', encoding='utf-8') as f:\n",
    "            c_lines = f.readlines()\n",
    "        for line in c_lines:\n",
    "            pattern = re.compile(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")  # 正则匹配保留中文字符\n",
    "            target = pattern.sub(\"\", line)\n",
    "            for data in jieba.lcut(target):\n",
    "                c_list.append(data)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{c_file}这个路径下没有文件\")\n",
    "\n",
    "    all_words = list(set(o_list).union(set(c_list)))\n",
    "    print(all_words)\n",
    "\n",
    "    la = []\n",
    "    lb = []\n",
    "    # 转换为向量的形式\n",
    "    for word in all_words:\n",
    "        la.append(o_list.count(word))\n",
    "        lb.append(c_list.count(word))\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    laa = np.array(la)\n",
    "    lbb = np.array(lb)\n",
    "    cos = (np.dot(laa, lbb.T)) / ((np.sqrt(np.dot(laa, laa.T))) * (np.sqrt(np.dot(lbb, lbb.T))))\n",
    "    print(f\"两篇文章的相似度为{cos}\")\n",
    "\n",
    "def long_analyse(fname):\n",
    "    try:\n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{fname}这个路径下并不存在文件\")\n",
    "\n",
    "    tags = jieba.analyse.extract_tags(content, 10)\n",
    "    return tags\n",
    "\n",
    "def compute_sim(o_file, c_file):\n",
    "    i = set(o_file).intersection(set(c_file))\n",
    "    j = set(o_file).union((set(c_file)))\n",
    "    return round(len(i)*100 / len(j), 2)\n",
    "\n",
    "def ans(o_file, c_file):\n",
    "    a1 = long_analyse(o_file)\n",
    "    a2 = long_analyse(c_file)\n",
    "    return compute_sim(a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3271f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    o_file = 'orig.txt'\n",
    "    c_file = 'orig_0.8_add.txt'\n",
    "    \n",
    "    # Call the short_analyse function\n",
    "    short_analyse(o_file, c_file)\n",
    "    \n",
    "    # Call the ans function\n",
    "    similarity = ans(o_file, c_file)\n",
    "    print(f\"The similarity between the papers is: {similarity}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
